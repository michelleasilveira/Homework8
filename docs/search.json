[
  {
    "objectID": "homework8.html",
    "href": "homework8.html",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "",
    "text": "set.seed(2025)\n\n# Core\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(skimr)\n\n# Viz\nlibrary(GGally)\n\n# Modeling\nlibrary(tidymodels)"
  },
  {
    "objectID": "homework8.html#plots-and-correlations",
    "href": "homework8.html#plots-and-correlations",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "Plots and correlations",
    "text": "Plots and correlations\n\n# Distribution of daily rentals\nggplot(daily, aes(bike_count)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Daily Bike Rentals\", x = \"Count (daily total)\")\n\n\n\n\n\n\n\n# Rentals by season\nggplot(daily, aes(seasons, bike_count, fill = seasons)) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(title = \"Bike rentals by season\")\n\n\n\n\n\n\n\n# Weather relationships\nggplot(daily, aes(temp, bike_count)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Temperature vs rentals\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(daily, aes(humidity, bike_count)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Humidity vs rentals\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nnum_tbl &lt;- daily %&gt;% select(where(is.numeric))\nround(cor(num_tbl, use = \"pairwise.complete.obs\"), 3)\n\n                bike_count rainfall snowfall   temp humidity windspeed\nbike_count           1.000   -0.239   -0.265  0.753    0.036    -0.193\nrainfall            -0.239    1.000   -0.023  0.145    0.529    -0.102\nsnowfall            -0.265   -0.023    1.000 -0.267    0.065     0.021\ntemp                 0.753    0.145   -0.267  1.000    0.404    -0.261\nhumidity             0.036    0.529    0.065  0.404    1.000    -0.234\nwindspeed           -0.193   -0.102    0.021 -0.261   -0.234     1.000\nvisibility           0.166   -0.222   -0.102  0.002   -0.559     0.206\ndew_point_temp       0.650    0.265   -0.210  0.963    0.632    -0.288\nsolar_radiation      0.736   -0.323   -0.233  0.550   -0.274     0.096\n                visibility dew_point_temp solar_radiation\nbike_count           0.166          0.650           0.736\nrainfall            -0.222          0.265          -0.323\nsnowfall            -0.102         -0.210          -0.233\ntemp                 0.002          0.963           0.550\nhumidity            -0.559          0.632          -0.274\nwindspeed            0.206         -0.288           0.096\nvisibility           1.000         -0.154           0.271\ndew_point_temp      -0.154          1.000           0.383\nsolar_radiation      0.271          0.383           1.000"
  },
  {
    "objectID": "homework8.html#best-mlr-model-from-hw8-baseline-for-comparison",
    "href": "homework8.html#best-mlr-model-from-hw8-baseline-for-comparison",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.1 Best MLR model from HW8 (baseline for comparison)",
    "text": "10.1 Best MLR model from HW8 (baseline for comparison)\n\n# Metric set for all new comparisons\nreg_metrics &lt;- metric_set(rmse, mae)\n\n# best_wf was defined earlier based on CV RMSE across the 3 recipes\nmlr_best_fit &lt;- fit(best_wf, data = train)\n\nmlr_test_pred &lt;- predict(mlr_best_fit, new_data = test) %&gt;%\n  bind_cols(test %&gt;% select(bike_count))\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\nmlr_test_metrics &lt;- reg_metrics(\n  mlr_test_pred,\n  truth   = bike_count,\n  estimate = .pred\n) %&gt;%\n  mutate(model = paste0(\"Best MLR (\", best_model_name, \")\"))\n\nmlr_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate model              \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n1 rmse    standard       3611. Best MLR (Recipe 3)\n2 mae     standard       2783. Best MLR (Recipe 3)\n\n\n\nmlr_best_coefs &lt;- extract_fit_parsnip(mlr_best_fit) %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate)))\n\nhead(mlr_best_coefs, 20)\n\n# A tibble: 20 × 5\n   term                   estimate std.error statistic  p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 dew_point_temp_poly_1   218202.    81355.     2.68  7.83e- 3\n 2 temp_poly_1            -156036.    69272.    -2.25  2.52e- 2\n 3 humidity_poly_1         -69789.    23982.    -2.91  3.96e- 3\n 4 temp_poly_2             -48427.    18083.    -2.68  7.93e- 3\n 5 solar_radiation_poly_1   44897.     5776.     7.77  2.38e-13\n 6 rainfall_poly_1         -30619.     8175.    -3.75  2.27e- 4\n 7 (Intercept)              19203.     1584.    12.1   1.37e-26\n 8 rainfall_poly_2          17061.     3531.     4.83  2.44e- 6\n 9 dew_point_temp_poly_2    14243.    12567.     1.13  2.58e- 1\n10 seasons_Summer            6198.      970.     6.39  8.96e-10\n11 humidity_poly_2          -5988.     5927.    -1.01  3.13e- 1\n12 seasons_Summer_x_temp    -5908.     1050.    -5.62  5.28e- 8\n13 windspeed_poly_1         -5708.     3404.    -1.68  9.49e- 2\n14 visibility_poly_1         5643.     4241.     1.33  1.85e- 1\n15 seasons_Winter           -5600.     1012.    -5.53  8.38e- 8\n16 snowfall_poly_2          -3508.     3146.    -1.12  2.66e- 1\n17 seasons_Winter_x_temp    -3385.     1300.    -2.60  9.80e- 3\n18 solar_radiation_poly_2   -3293.     3497.    -0.942 3.47e- 1\n19 seasons_Spring_x_temp     1862.      491.     3.79  1.89e- 4\n20 visibility_poly_2        -1849.     3043.    -0.607 5.44e- 1"
  },
  {
    "objectID": "homework8.html#lasso-using-recipe-1",
    "href": "homework8.html#lasso-using-recipe-1",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.2 LASSO (using Recipe 1)",
    "text": "10.2 LASSO (using Recipe 1)\n\nlasso_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 1          # 1 = pure LASSO\n) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_model(lasso_spec) %&gt;%\n  add_recipe(rec1)\n\n# Simple grid of penalty values\nlasso_grid &lt;- grid_regular(penalty(), levels = 30)\n\nset.seed(2025)\nlasso_tune &lt;- tune_grid(\n  lasso_wf,\n  resamples = cv10,\n  grid      = lasso_grid,\n  metrics   = reg_metrics\n)\n\ncollect_metrics(lasso_tune)\n\n# A tibble: 60 × 7\n    penalty .metric .estimator  mean     n std_err .config         \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1 1   e-10 mae     standard   3301.    10    171. pre0_mod01_post0\n 2 1   e-10 rmse    standard   4206.    10    239. pre0_mod01_post0\n 3 2.21e-10 mae     standard   3301.    10    171. pre0_mod02_post0\n 4 2.21e-10 rmse    standard   4206.    10    239. pre0_mod02_post0\n 5 4.89e-10 mae     standard   3301.    10    171. pre0_mod03_post0\n 6 4.89e-10 rmse    standard   4206.    10    239. pre0_mod03_post0\n 7 1.08e- 9 mae     standard   3301.    10    171. pre0_mod04_post0\n 8 1.08e- 9 rmse    standard   4206.    10    239. pre0_mod04_post0\n 9 2.40e- 9 mae     standard   3301.    10    171. pre0_mod05_post0\n10 2.40e- 9 rmse    standard   4206.    10    239. pre0_mod05_post0\n# ℹ 50 more rows\n\nlasso_best &lt;- select_best(lasso_tune, metric = \"rmse\")\nlasso_best\n\n# A tibble: 1 × 2\n       penalty .config         \n         &lt;dbl&gt; &lt;chr&gt;           \n1 0.0000000001 pre0_mod01_post0\n\n\n\nlasso_final_wf  &lt;- finalize_workflow(lasso_wf, lasso_best)\nlasso_final_fit &lt;- fit(lasso_final_wf, data = train)\n\nlasso_test_pred &lt;- predict(lasso_final_fit, new_data = test) %&gt;%\n  bind_cols(test %&gt;% select(bike_count))\n\nlasso_test_metrics &lt;- reg_metrics(\n  lasso_test_pred,\n  truth   = bike_count,\n  estimate = .pred\n) %&gt;%\n  mutate(model = \"LASSO\")\n\nlasso_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate model\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n1 rmse    standard       4276. LASSO\n2 mae     standard       3450. LASSO\n\n\n\nlasso_coefs &lt;- extract_fit_parsnip(lasso_final_fit) %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate)))\n\nhead(lasso_coefs, 20)\n\n# A tibble: 15 × 3\n   term                estimate      penalty\n   &lt;chr&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)         1.74e+ 4 0.0000000001\n 2 dew_point_temp      4.37e+ 3 0.0000000001\n 3 solar_radiation     3.96e+ 3 0.0000000001\n 4 rainfall           -2.20e+ 3 0.0000000001\n 5 seasons_Autumn      2.03e+ 3 0.0000000001\n 6 seasons_Winter     -1.31e+ 3 0.0000000001\n 7 humidity           -1.01e+ 3 0.0000000001\n 8 seasons_Summer      6.96e+ 2 0.0000000001\n 9 holiday_Holiday    -5.12e+ 2 0.0000000001\n10 windspeed          -4.85e+ 2 0.0000000001\n11 snowfall           -3.68e+ 2 0.0000000001\n12 seasons_Spring     -3.70e+ 0 0.0000000001\n13 holiday_No.Holiday  1.40e-10 0.0000000001\n14 temp                0        0.0000000001\n15 visibility          0        0.0000000001"
  },
  {
    "objectID": "homework8.html#regression-tree-tuned",
    "href": "homework8.html#regression-tree-tuned",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.3 Regression Tree (tuned)",
    "text": "10.3 Regression Tree (tuned)\n\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth      = tune(),\n  min_n           = tune()\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\ntree_wf &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;%\n  add_recipe(rec1)\n\ntree_grid &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(),\n  min_n(),\n  levels = c(5, 5, 5)\n)\n\nset.seed(2025)\ntree_tune &lt;- tune_grid(\n  tree_wf,\n  resamples = cv10,\n  grid      = tree_grid,\n  metrics   = reg_metrics\n)\n\ncollect_metrics(tree_tune)\n\n# A tibble: 250 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001          1     2 mae     standard   5106.    10    258.\n 2    0.0000000001          1     2 rmse    standard   6748.    10    337.\n 3    0.0000000001          1    11 mae     standard   5106.    10    258.\n 4    0.0000000001          1    11 rmse    standard   6748.    10    337.\n 5    0.0000000001          1    21 mae     standard   5106.    10    258.\n 6    0.0000000001          1    21 rmse    standard   6748.    10    337.\n 7    0.0000000001          1    30 mae     standard   5106.    10    258.\n 8    0.0000000001          1    30 rmse    standard   6748.    10    337.\n 9    0.0000000001          1    40 mae     standard   5106.    10    258.\n10    0.0000000001          1    40 rmse    standard   6748.    10    337.\n# ℹ 240 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\ntree_best &lt;- select_best(tree_tune, metric = \"rmse\")\ntree_best\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config          \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;            \n1        0.000562         11    21 pre0_mod093_post0\n\n\n\ntree_final_wf  &lt;- finalize_workflow(tree_wf, tree_best)\ntree_final_fit &lt;- fit(tree_final_wf, data = train)\n\ntree_test_pred &lt;- predict(tree_final_fit, new_data = test) %&gt;%\n  bind_cols(test %&gt;% select(bike_count))\n\ntree_test_metrics &lt;- reg_metrics(\n  tree_test_pred,\n  truth   = bike_count,\n  estimate = .pred\n) %&gt;%\n  mutate(model = \"Regression Tree\")\n\ntree_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate model          \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       4251. Regression Tree\n2 mae     standard       3130. Regression Tree\n\n\n\ntree_fit_parsnip &lt;- extract_fit_parsnip(tree_final_fit)\nrpart.plot::rpart.plot(tree_fit_parsnip$fit)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE."
  },
  {
    "objectID": "homework8.html#bagged-trees-tuned",
    "href": "homework8.html#bagged-trees-tuned",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.4 Bagged Trees (tuned)",
    "text": "10.4 Bagged Trees (tuned)\n\nbagged_spec &lt;- bag_tree(\n  cost_complexity = tune(),\n  min_n           = tune()\n) %&gt;%\n  set_engine(\"rpart\", times = 50) %&gt;%  # 50 bootstrap samples\n  set_mode(\"regression\")\n\nbagged_wf &lt;- workflow() %&gt;%\n  add_model(bagged_spec) %&gt;%\n  add_recipe(rec1)\n\nbagged_grid &lt;- grid_regular(\n  cost_complexity(),\n  min_n(),\n  levels = c(5, 5)\n)\n\nset.seed(2025)\nbagged_tune &lt;- tune_grid(\n  bagged_wf,\n  resamples = cv10,\n  grid      = bagged_grid,\n  metrics   = reg_metrics\n)\n\n→ A | warning: There was 1 warning in `dplyr::mutate()`.\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! package 'future' was built under R version 4.4.3\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\ncollect_metrics(bagged_tune)\n\n# A tibble: 50 × 8\n   cost_complexity min_n .metric .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1    0.0000000001     2 mae     standard   2330.    10    129. pre0_mod01_post0\n 2    0.0000000001     2 rmse    standard   2907.    10    168. pre0_mod01_post0\n 3    0.0000000001    11 mae     standard   2322.    10    142. pre0_mod02_post0\n 4    0.0000000001    11 rmse    standard   2930.    10    155. pre0_mod02_post0\n 5    0.0000000001    21 mae     standard   2548.    10    128. pre0_mod03_post0\n 6    0.0000000001    21 rmse    standard   3157.    10    140. pre0_mod03_post0\n 7    0.0000000001    30 mae     standard   2723.    10    173. pre0_mod04_post0\n 8    0.0000000001    30 rmse    standard   3370.    10    164. pre0_mod04_post0\n 9    0.0000000001    40 mae     standard   2793.    10    183. pre0_mod05_post0\n10    0.0000000001    40 rmse    standard   3425.    10    181. pre0_mod05_post0\n# ℹ 40 more rows\n\nbagged_best &lt;- select_best(bagged_tune, metric = \"rmse\")\nbagged_best\n\n# A tibble: 1 × 3\n  cost_complexity min_n .config         \n            &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           \n1      0.00000316     2 pre0_mod11_post0\n\n\n\nbagged_final_wf  &lt;- finalize_workflow(bagged_wf, bagged_best)\nbagged_final_fit &lt;- fit(bagged_final_wf, data = train)\n\nbagged_test_pred &lt;- predict(bagged_final_fit, new_data = test) %&gt;%\n  bind_cols(test %&gt;% select(bike_count))\n\nbagged_test_metrics &lt;- reg_metrics(\n  bagged_test_pred,\n  truth   = bike_count,\n  estimate = .pred\n) %&gt;%\n  mutate(model = \"Bagged Trees\")\n\nbagged_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate model       \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       \n1 rmse    standard       3185. Bagged Trees\n2 mae     standard       2469. Bagged Trees\n\n\n\n# Permutation-based variable importance for bagged trees\n\n# Prep recipe and bake training data\nrec1_prep &lt;- prep(rec1, training = train)\nbaked_train &lt;- bake(rec1_prep, new_data = train)\nX &lt;- baked_train %&gt;% dplyr::select(-bike_count)\ny &lt;- baked_train$bike_count\n\n# Custom prediction wrapper that handles the parameter name issue\nbagged_pred_wrapper &lt;- function(object, newdata) {\n  # The bagger needs 'new_data' with underscore\n  pred_result &lt;- predict(object, new_data = as.data.frame(newdata))\n  as.numeric(pred_result$.pred)\n}\n\n# Use the parsnip fit object\nbagged_fit_parsnip &lt;- extract_fit_parsnip(bagged_final_fit)\n\n# Permutation VIP\nvip::vip(\n  bagged_fit_parsnip,\n  method = \"permute\",\n  target = y,\n  train = as.data.frame(X),  # Ensure it's a data frame\n  metric = yardstick::rmse_vec,\n  pred_wrapper = bagged_pred_wrapper,\n  nsim = 10,\n  smaller_is_better = TRUE\n)"
  },
  {
    "objectID": "homework8.html#random-forest-tuned",
    "href": "homework8.html#random-forest-tuned",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.5 Random Forest (tuned)",
    "text": "10.5 Random Forest (tuned)\n\nrf_spec &lt;- rand_forest(\n  mtry  = tune(),\n  min_n = tune(),\n  trees = 500\n) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"regression\")\n\n# set up mtry range based on predictors\nrf_mtry &lt;- finalize(mtry(), train %&gt;% select(-bike_count))\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(rec1)\n\nrf_grid &lt;- grid_random(\n  rf_mtry,\n  min_n(),\n  size = 20\n)\n\nset.seed(2025)\nrf_tune &lt;- tune_grid(\n  rf_wf,\n  resamples = cv10,\n  grid      = rf_grid,\n  metrics   = reg_metrics\n)\n\ncollect_metrics(rf_tune)\n\n# A tibble: 38 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1     1    17 mae     standard   4277.    10    305. pre0_mod01_post0\n 2     1    17 rmse    standard   5069.    10    327. pre0_mod01_post0\n 3     1    38 mae     standard   4425.    10    287. pre0_mod02_post0\n 4     1    38 rmse    standard   5250.    10    310. pre0_mod02_post0\n 5     2    31 mae     standard   3183.    10    266. pre0_mod03_post0\n 6     2    31 rmse    standard   3870.    10    293. pre0_mod03_post0\n 7     3     2 mae     standard   2422.    10    189. pre0_mod04_post0\n 8     3     2 rmse    standard   3006.    10    212. pre0_mod04_post0\n 9     3    14 mae     standard   2631.    10    215. pre0_mod05_post0\n10     3    14 rmse    standard   3220.    10    235. pre0_mod05_post0\n# ℹ 28 more rows\n\nrf_best &lt;- select_best(rf_tune, metric = \"rmse\")\nrf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     9     3 pre0_mod15_post0\n\n\n\nrf_final_wf  &lt;- finalize_workflow(rf_wf, rf_best)\nrf_final_fit &lt;- fit(rf_final_wf, data = train)\n\nrf_test_pred &lt;- predict(rf_final_fit, new_data = test) %&gt;%\n  bind_cols(test %&gt;% select(bike_count))\n\nrf_test_metrics &lt;- reg_metrics(\n  rf_test_pred,\n  truth   = bike_count,\n  estimate = .pred\n) %&gt;%\n  mutate(model = \"Random Forest\")\n\nrf_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate model        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        \n1 rmse    standard       3167. Random Forest\n2 mae     standard       2466. Random Forest\n\n\n\nrf_fit_parsnip &lt;- extract_fit_parsnip(rf_final_fit)\nvip::vip(rf_fit_parsnip$fit)"
  },
  {
    "objectID": "homework8.html#compare-all-final-models-on-the-test-set",
    "href": "homework8.html#compare-all-final-models-on-the-test-set",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.6 Compare all final models on the test set",
    "text": "10.6 Compare all final models on the test set\n\nall_test_metrics &lt;- bind_rows(\n  mlr_test_metrics,\n  lasso_test_metrics,\n  tree_test_metrics,\n  bagged_test_metrics,\n  rf_test_metrics\n) %&gt;%\n  filter(.metric %in% c(\"rmse\", \"mae\")) %&gt;%\n  arrange(.metric, .estimate)  # Changed 'mean' to '.estimate'\n\nall_test_metrics\n\n# A tibble: 10 × 4\n   .metric .estimator .estimate model              \n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n 1 mae     standard       2466. Random Forest      \n 2 mae     standard       2469. Bagged Trees       \n 3 mae     standard       2783. Best MLR (Recipe 3)\n 4 mae     standard       3130. Regression Tree    \n 5 mae     standard       3450. LASSO              \n 6 rmse    standard       3167. Random Forest      \n 7 rmse    standard       3185. Bagged Trees       \n 8 rmse    standard       3611. Best MLR (Recipe 3)\n 9 rmse    standard       4251. Regression Tree    \n10 rmse    standard       4276. LASSO              \n\n\n\nbest_overall &lt;- all_test_metrics %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(.estimate) %&gt;%  # Changed 'mean' to '.estimate'\n  slice(1)\n\nbest_overall\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        \n1 rmse    standard       3167. Random Forest\n\noverall_best_name &lt;- best_overall$model[[1]]\noverall_best_name\n\n[1] \"Random Forest\""
  },
  {
    "objectID": "homework8.html#fit-the-overall-best-model-on-the-entire-data-set",
    "href": "homework8.html#fit-the-overall-best-model-on-the-entire-data-set",
    "title": "Homework 9 - Seoul Bike Rentals — Additional MLR Models",
    "section": "10.7 Fit the overall best model on the entire data set",
    "text": "10.7 Fit the overall best model on the entire data set\n\noverall_best_wf &lt;- switch(\n  overall_best_name,\n  \"Best MLR (Recipe 3)\" = best_wf,\n  \"LASSO\"               = lasso_final_wf,\n  \"Regression Tree\"     = tree_final_wf,\n  \"Bagged Trees\"        = bagged_final_wf,\n  \"Random Forest\"       = rf_final_wf\n)\n\nbest_full_fit &lt;- fit(overall_best_wf, data = daily)\nbest_full_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), num.trees = ~500, min.node.size = min_rows(~3L, x), importance = ~\"permutation\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  14 \nMtry:                             9 \nTarget node size:                 3 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       8021746 \nR squared (OOB):                  0.9187649"
  }
]